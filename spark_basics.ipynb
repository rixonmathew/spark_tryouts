{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize spark and run basic commands to query local json and csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/06 07:06:01 WARN Utils: Your hostname, RixM1Mini.local resolves to a loopback address: 127.0.0.1; using 192.168.1.20 instead (on interface en0)\n",
      "23/01/06 07:06:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/usr/local/spark-3.3.0-bin-hadoop3/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/rixonmathew/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/rixonmathew/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.3_2.12 added as a dependency\n",
      "software.amazon.awssdk#bundle added as a dependency\n",
      "software.amazon.awssdk#url-connection-client added as a dependency\n",
      "org.apache.spark#spark-hadoop-cloud_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-b83aaac8-7397-49ff-8466-791db228d9af;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.3_2.12;1.1.0 in central\n",
      "\tfound software.amazon.awssdk#bundle;2.17.257 in central\n",
      "\tfound software.amazon.eventstream#eventstream;1.0.1 in central\n",
      "\tfound software.amazon.awssdk#url-connection-client;2.17.257 in central\n",
      "\tfound software.amazon.awssdk#utils;2.17.257 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in local-m2-cache\n",
      "\tfound software.amazon.awssdk#annotations;2.17.257 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in local-m2-cache\n",
      "\tfound software.amazon.awssdk#http-client-spi;2.17.257 in central\n",
      "\tfound software.amazon.awssdk#metrics-spi;2.17.257 in central\n",
      "\tfound org.apache.spark#spark-hadoop-cloud_2.12;3.3.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.2 in local-m2-cache\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.2 in local-m2-cache\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in local-m2-cache\n",
      "\tfound org.slf4j#slf4j-api;1.7.32 in local-m2-cache\n",
      "\tfound commons-logging#commons-logging;1.1.3 in local-m2-cache\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in local-m2-cache\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.1026 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound org.apache.hadoop#hadoop-openstack;3.3.2 in central\n",
      "\tfound org.apache.hadoop#hadoop-annotations;3.3.2 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.14 in local-m2-cache\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.13.4 in local-m2-cache\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.13.4.1 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.13.4 in local-m2-cache\n",
      "\tfound joda-time#joda-time;2.10.13 in central\n",
      "\tfound com.fasterxml.jackson.dataformat#jackson-dataformat-cbor;2.13.4 in local-m2-cache\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in local-m2-cache\n",
      "\tfound commons-codec#commons-codec;1.15 in local-m2-cache\n",
      "\tfound org.apache.hadoop#hadoop-azure;3.3.2 in central\n",
      "\tfound com.microsoft.azure#azure-storage;7.0.1 in central\n",
      "\tfound com.microsoft.azure#azure-keyvault-core;1.0.0 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 in central\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in local-m2-cache\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in local-m2-cache\n",
      "\tfound org.apache.hadoop#hadoop-cloud-storage;3.3.2 in central\n",
      "\tfound org.apache.hadoop#hadoop-aliyun;3.3.2 in central\n",
      "\tfound com.aliyun.oss#aliyun-sdk-oss;3.13.0 in central\n",
      "\tfound org.jdom#jdom2;2.0.6 in local-m2-cache\n",
      "\tfound org.codehaus.jettison#jettison;1.1 in central\n",
      "\tfound stax#stax-api;1.0.1 in local-m2-cache\n",
      "\tfound com.aliyun#aliyun-java-sdk-core;4.5.10 in central\n",
      "\tfound com.google.code.gson#gson;2.8.9 in local-m2-cache\n",
      "\tfound javax.xml.bind#jaxb-api;2.2.11 in local-m2-cache\n",
      "\tfound org.ini4j#ini4j;0.5.4 in local-m2-cache\n",
      "\tfound io.opentracing#opentracing-api;0.33.0 in central\n",
      "\tfound io.opentracing#opentracing-util;0.33.0 in central\n",
      "\tfound io.opentracing#opentracing-noop;0.33.0 in central\n",
      "\tfound com.aliyun#aliyun-java-sdk-ram;3.1.0 in central\n",
      "\tfound com.aliyun#aliyun-java-sdk-kms;2.11.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-azure-datalake;3.3.2 in central\n",
      "\tfound com.microsoft.azure#azure-data-lake-store-sdk;2.3.9 in central\n",
      "\tfound org.apache.hadoop#hadoop-cos;3.3.2 in central\n",
      "\tfound com.qcloud#cos_api-bundle;5.6.19 in central\n",
      "\tfound org.eclipse.jetty#jetty-util-ajax;9.4.48.v20220622 in central\n",
      "\tfound org.eclipse.jetty#jetty-util;9.4.48.v20220622 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in local-m2-cache\n",
      ":: resolution report :: resolve 13174ms :: artifacts dl 37ms\n",
      "\t:: modules in use:\n",
      "\tcom.aliyun#aliyun-java-sdk-core;4.5.10 from central in [default]\n",
      "\tcom.aliyun#aliyun-java-sdk-kms;2.11.0 from central in [default]\n",
      "\tcom.aliyun#aliyun-java-sdk-ram;3.1.0 from central in [default]\n",
      "\tcom.aliyun.oss#aliyun-sdk-oss;3.13.0 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.1026 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.13.4 from local-m2-cache in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.13.4 from local-m2-cache in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.13.4.1 from central in [default]\n",
      "\tcom.fasterxml.jackson.dataformat#jackson-dataformat-cbor;2.13.4 from local-m2-cache in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from local-m2-cache in [default]\n",
      "\tcom.google.code.gson#gson;2.8.9 from local-m2-cache in [default]\n",
      "\tcom.microsoft.azure#azure-data-lake-store-sdk;2.3.9 from central in [default]\n",
      "\tcom.microsoft.azure#azure-keyvault-core;1.0.0 from central in [default]\n",
      "\tcom.microsoft.azure#azure-storage;7.0.1 from central in [default]\n",
      "\tcom.qcloud#cos_api-bundle;5.6.19 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from local-m2-cache in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from local-m2-cache in [default]\n",
      "\tio.opentracing#opentracing-api;0.33.0 from central in [default]\n",
      "\tio.opentracing#opentracing-noop;0.33.0 from central in [default]\n",
      "\tio.opentracing#opentracing-util;0.33.0 from central in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.2.11 from local-m2-cache in [default]\n",
      "\tjoda-time#joda-time;2.10.13 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aliyun;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-annotations;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-azure;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-azure-datalake;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.2 from local-m2-cache in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.2 from local-m2-cache in [default]\n",
      "\torg.apache.hadoop#hadoop-cloud-storage;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-cos;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-openstack;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from local-m2-cache in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.14 from local-m2-cache in [default]\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.3_2.12;1.1.0 from central in [default]\n",
      "\torg.apache.spark#spark-hadoop-cloud_2.12;3.3.1 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from local-m2-cache in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from local-m2-cache in [default]\n",
      "\torg.codehaus.jettison#jettison;1.1 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util;9.4.48.v20220622 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.4.48.v20220622 from central in [default]\n",
      "\torg.ini4j#ini4j;0.5.4 from local-m2-cache in [default]\n",
      "\torg.jdom#jdom2;2.0.6 from local-m2-cache in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from local-m2-cache in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.32 from local-m2-cache in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from local-m2-cache in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from local-m2-cache in [default]\n",
      "\tsoftware.amazon.awssdk#annotations;2.17.257 from central in [default]\n",
      "\tsoftware.amazon.awssdk#bundle;2.17.257 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-client-spi;2.17.257 from central in [default]\n",
      "\tsoftware.amazon.awssdk#metrics-spi;2.17.257 from central in [default]\n",
      "\tsoftware.amazon.awssdk#url-connection-client;2.17.257 from central in [default]\n",
      "\tsoftware.amazon.awssdk#utils;2.17.257 from central in [default]\n",
      "\tsoftware.amazon.eventstream#eventstream;1.0.1 from central in [default]\n",
      "\tstax#stax-api;1.0.1 from local-m2-cache in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.slf4j#slf4j-api;1.7.30 by [org.slf4j#slf4j-api;1.7.32] in [default]\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 by [org.eclipse.jetty#jetty-util-ajax;9.4.48.v20220622] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   59  |   6   |   6   |   2   ||   57  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      "\n",
      ":: problems summary ::\n",
      ":::: ERRORS\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\n",
      ":: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-b83aaac8-7397-49ff-8466-791db228d9af\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 57 already retrieved (0kB/28ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/06 07:06:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "# When connecting to a spark master ensure max limits are specified to avoid resource wait due to starvations\n",
    "# spark = SparkSession \\\n",
    "#   .builder \\\n",
    "#   .master(\"spark://rixp330-ubuntu.greyhound-cloud.ts.net:7077\") \\\n",
    "#   .appName(\"LearningSpark\") \\\n",
    "#   .config(\"spark.cores.max\",\"1\") \\\n",
    "#   .getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"LearningSparkLocal\") \\\n",
    "  .config(\"spark.cores.max\",2) \\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-----+---+-------+---------+-----------------+\n",
      "|           Campaigns|    First| Hits| Id|   Last|Published|              Url|\n",
      "+--------------------+---------+-----+---+-------+---------+-----------------+\n",
      "| [twitter, LinkedIn]|    Jules| 4535|  1|  Damji| 1/4/2016|https://tinyurl.1|\n",
      "| [twitter, LinkedIn]|   Brooke| 8908|  2|  Wenig| 5/5/2018|https://tinyurl.2|\n",
      "|[web, twitter, FB...|    Denny| 7659|  3|    Lee| 6/7/2019|https://tinyurl.3|\n",
      "|       [twitter, FB]|Tathagata|10568|  4|    Das|5/12/2018|https://tinyurl.4|\n",
      "|[web, twitter, FB...|    Matei|40578|  5|Zaharia|5/14/2014|https://tinyurl.5|\n",
      "| [twitter, LinkedIn]|  Reynold|25568|  6|    Xin| 3/2/2015|https://tinyurl.6|\n",
      "+--------------------+---------+-----+---+-------+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_json_df = spark.read.json(\"test.json\")\n",
    "test_json_df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read CSV files from local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+\n",
      "|CallNumber|UnitID|IncidentNumber|        CallType|  CallDate| WatchDate|CallFinalDisposition|       AvailableDtTm|\n",
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+\n",
      "|  20110016|   T13|       2003235|  Structure Fire|01/11/2002|01/10/2002|               Other|01/11/2002 01:51:...|\n",
      "|  20110022|   M17|       2003241|Medical Incident|01/11/2002|01/10/2002|               Other|01/11/2002 03:01:...|\n",
      "|  20110023|   M41|       2003242|Medical Incident|01/11/2002|01/10/2002|               Other|01/11/2002 02:39:...|\n",
      "|  20110032|   E11|       2003250|    Vehicle Fire|01/11/2002|01/10/2002|               Other|01/11/2002 04:16:...|\n",
      "|  20110043|   B04|       2003259|          Alarms|01/11/2002|01/10/2002|               Other|01/11/2002 06:01:...|\n",
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_csv_df = spark.read.option(\"header\",\"true\").csv(\"test.csv\")\n",
    "test_csv_df.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a view from json file and use sql to query that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+--------------------+\n",
      "| id|              url|           campaigns|\n",
      "+---+-----------------+--------------------+\n",
      "|  1|https://tinyurl.1| [twitter, LinkedIn]|\n",
      "|  2|https://tinyurl.2| [twitter, LinkedIn]|\n",
      "|  3|https://tinyurl.3|[web, twitter, FB...|\n",
      "|  4|https://tinyurl.4|       [twitter, FB]|\n",
      "|  5|https://tinyurl.5|[web, twitter, FB...|\n",
      "|  6|https://tinyurl.6| [twitter, LinkedIn]|\n",
      "+---+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.json(\"test.json\").createOrReplaceTempView(\"blogs\")\n",
    "results = spark.sql(\"select id,url,campaigns from blogs\")\n",
    "results.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read CSV files from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/06 07:06:51 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "23/01/06 07:06:51 WARN BasicProfileConfigLoader: Your profile name includes a 'profile ' prefix. This is considered part of the profile name in the Java SDK, so you will need to include this prefix in your profile name when you reference this profile from your Java code.\n",
      "23/01/06 07:06:51 WARN BasicProfileConfigLoader: Your profile name includes a 'profile ' prefix. This is considered part of the profile name in the Java SDK, so you will need to include this prefix in your profile name when you reference this profile from your Java code.\n",
      "23/01/06 07:06:51 WARN BasicProfileConfigLoader: Your profile name includes a 'profile ' prefix. This is considered part of the profile name in the Java SDK, so you will need to include this prefix in your profile name when you reference this profile from your Java code.\n",
      "23/01/06 07:06:51 WARN BasicProfileConfigLoader: Your profile name includes a 'profile ' prefix. This is considered part of the profile name in the Java SDK, so you will need to include this prefix in your profile name when you reference this profile from your Java code.\n",
      "23/01/06 07:06:51 WARN BasicProfileConfigLoader: Your profile name includes a 'profile ' prefix. This is considered part of the profile name in the Java SDK, so you will need to include this prefix in your profile name when you reference this profile from your Java code.\n",
      "23/01/06 07:06:51 WARN BasicProfileConfigLoader: Your profile name includes a 'profile ' prefix. This is considered part of the profile name in the Java SDK, so you will need to include this prefix in your profile name when you reference this profile from your Java code.\n",
      "23/01/06 07:06:51 WARN BasicProfileConfigLoader: Your profile name includes a 'profile ' prefix. This is considered part of the profile name in the Java SDK, so you will need to include this prefix in your profile name when you reference this profile from your Java code.\n",
      "23/01/06 07:06:51 WARN BasicProfileConfigLoader: Your profile name includes a 'profile ' prefix. This is considered part of the profile name in the Java SDK, so you will need to include this prefix in your profile name when you reference this profile from your Java code.\n",
      "23/01/06 07:06:51 WARN BasicProfileConfigLoader: Your profile name includes a 'profile ' prefix. This is considered part of the profile name in the Java SDK, so you will need to include this prefix in your profile name when you reference this profile from your Java code.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---------------+---+-----+\n",
      "|_c0|                 _c1|            _c2|_c3|  _c4|\n",
      "+---+--------------------+---------------+---+-----+\n",
      "|  1|         Toyota Park|     Bridgeview| IL|    0|\n",
      "|  2|Columbus Crew Sta...|       Columbus| OH|    0|\n",
      "|  3|         RFK Stadium|     Washington| DC|    0|\n",
      "|  4|CommunityAmerica ...|    Kansas City| KS|    0|\n",
      "|  5|    Gillette Stadium|     Foxborough| MA|68756|\n",
      "|  6|New York Giants S...|East Rutherford| NJ|80242|\n",
      "|  7|           BMO Field|        Toronto| ON|    0|\n",
      "|  8|The Home Depot Ce...|         Carson| CA|    0|\n",
      "|  9|Dick's Sporting G...|  Commerce City| CO|    0|\n",
      "| 10|      Pizza Hut Park|         Frisco| TX|    0|\n",
      "| 11|   Robertson Stadium|        Houston| TX|    0|\n",
      "| 13| Rice-Eccles Stadium| Salt Lake City| UT|    0|\n",
      "| 14|   Buck Shaw Stadium|    Santa Clara| CA|    0|\n",
      "| 15|     McAfee Coliseum|        Oakland| CA|63026|\n",
      "| 16| TD Banknorth Garden|         Boston| MA|    0|\n",
      "| 17|         Izod Center|East Rutherford| NJ|    0|\n",
      "| 18|Madison Square Ga...|  New York City| NY|20000|\n",
      "| 19|     Wachovia Center|   Philadelphia| PA|    0|\n",
      "| 20|   Air Canada Centre|        Toronto| ON|    0|\n",
      "| 21|       United Center|        Chicago| IL|    0|\n",
      "+---+--------------------+---------------+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "csvS3 = spark.read.format('csv').options(header='false',inferSchema='false',delimiter='|').load('s3a://data-lake-demo-rixon/tickitdb/venue/venue_pipe.txt')\n",
    "csvS3.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_tryouts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e6071525427c47e2866e7d82ca6053209ee0fe2618217df85791c9112205ab6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
