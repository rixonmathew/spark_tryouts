{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize spark and run basic commands to query local json and csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "# When connecting to a spark master ensure max limits are specified to avoid resource wait due to starvations\n",
    "spark = SparkSession \\\n",
    "  .builder \\\n",
    "  .master(\"spark://rixp330-ubuntu:7077\") \\\n",
    "  .appName(\"LearningSpark\") \\\n",
    "  .config(\"spark.cores.max\",\"1\") \\\n",
    "  .getOrCreate()\n",
    "\n",
    "# spark = SparkSession.builder.master(\"local[10]\").appName(\"LearningSparkLocal\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-----+---+-------+---------+-----------------+\n",
      "|           Campaigns|    First| Hits| Id|   Last|Published|              Url|\n",
      "+--------------------+---------+-----+---+-------+---------+-----------------+\n",
      "| [twitter, LinkedIn]|    Jules| 4535|  1|  Damji| 1/4/2016|https://tinyurl.1|\n",
      "| [twitter, LinkedIn]|   Brooke| 8908|  2|  Wenig| 5/5/2018|https://tinyurl.2|\n",
      "|[web, twitter, FB...|    Denny| 7659|  3|    Lee| 6/7/2019|https://tinyurl.3|\n",
      "|       [twitter, FB]|Tathagata|10568|  4|    Das|5/12/2018|https://tinyurl.4|\n",
      "|[web, twitter, FB...|    Matei|40578|  5|Zaharia|5/14/2014|https://tinyurl.5|\n",
      "| [twitter, LinkedIn]|  Reynold|25568|  6|    Xin| 3/2/2015|https://tinyurl.6|\n",
      "+--------------------+---------+-----+---+-------+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_json_df = spark.read.json(\"test.json\")\n",
    "test_json_df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read CSV files from local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+\n",
      "|CallNumber|UnitID|IncidentNumber|        CallType|  CallDate| WatchDate|CallFinalDisposition|       AvailableDtTm|\n",
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+\n",
      "|  20110016|   T13|       2003235|  Structure Fire|01/11/2002|01/10/2002|               Other|01/11/2002 01:51:...|\n",
      "|  20110022|   M17|       2003241|Medical Incident|01/11/2002|01/10/2002|               Other|01/11/2002 03:01:...|\n",
      "|  20110023|   M41|       2003242|Medical Incident|01/11/2002|01/10/2002|               Other|01/11/2002 02:39:...|\n",
      "|  20110032|   E11|       2003250|    Vehicle Fire|01/11/2002|01/10/2002|               Other|01/11/2002 04:16:...|\n",
      "|  20110043|   B04|       2003259|          Alarms|01/11/2002|01/10/2002|               Other|01/11/2002 06:01:...|\n",
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_csv_df = spark.read.option(\"header\",\"true\").csv(\"test.csv\")\n",
    "test_csv_df.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a view from json file and use sql to query that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+--------------------+\n",
      "| id|              url|           campaigns|\n",
      "+---+-----------------+--------------------+\n",
      "|  1|https://tinyurl.1| [twitter, LinkedIn]|\n",
      "|  2|https://tinyurl.2| [twitter, LinkedIn]|\n",
      "|  3|https://tinyurl.3|[web, twitter, FB...|\n",
      "|  4|https://tinyurl.4|       [twitter, FB]|\n",
      "|  5|https://tinyurl.5|[web, twitter, FB...|\n",
      "|  6|https://tinyurl.6| [twitter, LinkedIn]|\n",
      "+---+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.json(\"test.json\").createOrReplaceTempView(\"blogs\")\n",
    "results = spark.sql(\"select id,url,campaigns from blogs\")\n",
    "results.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read CSV files from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---------------+---+-----+\n",
      "|_c0|                 _c1|            _c2|_c3|  _c4|\n",
      "+---+--------------------+---------------+---+-----+\n",
      "|  1|         Toyota Park|     Bridgeview| IL|    0|\n",
      "|  2|Columbus Crew Sta...|       Columbus| OH|    0|\n",
      "|  3|         RFK Stadium|     Washington| DC|    0|\n",
      "|  4|CommunityAmerica ...|    Kansas City| KS|    0|\n",
      "|  5|    Gillette Stadium|     Foxborough| MA|68756|\n",
      "|  6|New York Giants S...|East Rutherford| NJ|80242|\n",
      "|  7|           BMO Field|        Toronto| ON|    0|\n",
      "|  8|The Home Depot Ce...|         Carson| CA|    0|\n",
      "|  9|Dick's Sporting G...|  Commerce City| CO|    0|\n",
      "| 10|      Pizza Hut Park|         Frisco| TX|    0|\n",
      "| 11|   Robertson Stadium|        Houston| TX|    0|\n",
      "| 13| Rice-Eccles Stadium| Salt Lake City| UT|    0|\n",
      "| 14|   Buck Shaw Stadium|    Santa Clara| CA|    0|\n",
      "| 15|     McAfee Coliseum|        Oakland| CA|63026|\n",
      "| 16| TD Banknorth Garden|         Boston| MA|    0|\n",
      "| 17|         Izod Center|East Rutherford| NJ|    0|\n",
      "| 18|Madison Square Ga...|  New York City| NY|20000|\n",
      "| 19|     Wachovia Center|   Philadelphia| PA|    0|\n",
      "| 20|   Air Canada Centre|        Toronto| ON|    0|\n",
      "| 21|       United Center|        Chicago| IL|    0|\n",
      "+---+--------------------+---------------+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csvS3 = spark.read.format('csv').options(header='false',inferSchema='false',delimiter='|').load('s3a://data-lake-demo-rixon/tickitdb/venue/venue_pipe.txt')\n",
    "csvS3.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_tryouts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9dcfdf006dc17c5059b564b76f71caeca8b6bf7cbf312b59f6f6bdd31ffed832"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
